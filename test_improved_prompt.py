"""
Test script to compare original vs improved prompt performance
"""

import asyncio
import json
import aiohttp
from typing import Dict, Any
from hr_analytics_prompt_improved import get_improved_prompt

async def test_question_with_improved_prompt(question: str) -> Dict[str, Any]:
    """Test a single question with the improved prompt"""
    
    # Get improved prompt
    improved_prompt = get_improved_prompt(use_local_cache=True)
    
    payload = {
        "message": question,
        "use_local_cache": True,
        "custom_prompt": improved_prompt  # If the bot supports custom prompts
    }
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.post("http://localhost:8001/chat", json=payload) as response:
                if response.status == 200:
                    result = await response.json()
                    return result
                else:
                    error_text = await response.text()
                    return {"error": f"HTTP {response.status}: {error_text}"}
        except Exception as e:
            return {"error": str(e)}

async def compare_prompts():
    """Compare original vs improved prompt on key test questions"""
    
    test_questions = [
        "–°–∫–æ–ª—å–∫–æ —É –Ω–∞—Å —Å–µ–π—á–∞—Å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ –≤–æ—Ä–æ–Ω–∫–µ?",
        "–°–∫–æ–ª—å–∫–æ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —É –Ω–∞—Å –µ—Å—Ç—å?", 
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞—à–∏ —Ä–µ–∫—Ä—É—Ç–µ—Ä—ã?"
    ]
    
    print("üß™ Testing Improved Prompt")
    print("=" * 50)
    
    for question in test_questions:
        print(f"\nüìã Question: {question}")
        print("-" * 30)
        
        # Test with current system (assuming it uses original prompt)
        result = await test_question_with_improved_prompt(question)
        
        if "response" in result and isinstance(result["response"], str):
            try:
                parsed_response = json.loads(result["response"])
                
                # Check key structure elements
                has_main_metric = "main_metric" in parsed_response
                has_secondary_metrics = "secondary_metrics" in parsed_response
                secondary_count = len(parsed_response.get("secondary_metrics", []))
                entity_name = parsed_response.get("main_metric", {}).get("value", {}).get("entity", "unknown")
                
                print(f"‚úÖ Main metric: {has_main_metric}")
                print(f"‚úÖ Secondary metrics: {has_secondary_metrics} (count: {secondary_count})")
                print(f"‚úÖ Entity used: {entity_name}")
                
                # Check if using systematic naming
                systematic_entities = [
                    "applicants_by_status", "applicants_all", "vacancies_open", 
                    "vacancies_all", "actions_by_recruiter", "moves_by_recruiter"
                ]
                uses_systematic_naming = entity_name in systematic_entities
                print(f"‚úÖ Systematic naming: {uses_systematic_naming}")
                
                if not uses_systematic_naming:
                    print(f"‚ö†Ô∏è  Should use systematic naming instead of: {entity_name}")
                
            except json.JSONDecodeError:
                print("‚ùå Invalid JSON response")
        elif "error" in result:
            print(f"‚ùå Error: {result['error']}")
        else:
            print("‚ùå Unexpected response format")

if __name__ == "__main__":
    asyncio.run(compare_prompts())