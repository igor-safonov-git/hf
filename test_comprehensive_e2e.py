#!/usr/bin/env python3
"""
Comprehensive End-to-End Test for Unified Metrics Group By
Phase 6.1: Run comprehensive end-to-end test with real data to verify complete functionality
"""

import asyncio
import json
import time
from chart_data_processor import process_chart_data, validate_report_json
from huntflow_local_client import HuntflowLocalClient

# Test scenarios covering all major use cases
test_scenarios = [
    {
        "name": "Recruiter Performance Analysis",
        "description": "Individual recruiter metrics with monthly trends",
        "report": {
            "report_title": "–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–µ–∫—Ä—É—Ç–µ—Ä–æ–≤",
            "period": "6 month",
            "metrics_group_by": "recruiters",
            "main_metric": {
                "label": "–ù–∞–Ω—è—Ç–æ —Ä–µ–∫—Ä—É—Ç–µ—Ä–æ–º",
                "value": {
                    "operation": "count",
                    "entity": "hires",
                    "value_field": None,
                    "filters": {"period": "6 month"}
                }
            },
            "secondary_metrics": [
                {
                    "label": "–ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ",
                    "value": {
                        "operation": "count",
                        "entity": "applicants",
                        "value_field": None,
                        "filters": {"period": "6 month"}
                    }
                },
                {
                    "label": "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞–π–º–∞",
                    "value": {
                        "operation": "avg",
                        "entity": "hires",
                        "value_field": "time_to_hire",
                        "filters": {"period": "6 month"}
                    }
                }
            ],
            "chart": {
                "label": "–î–∏–Ω–∞–º–∏–∫–∞ –Ω–∞–π–º–∞ –ø–æ –º–µ—Å—è—Ü–∞–º",
                "type": "line",
                "x_label": "–ú–µ—Å—è—Ü",
                "y_label": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–º–∞",
                "x_axis": {
                    "operation": "count",
                    "entity": "hires",
                    "group_by": {"field": "month"},
                    "filters": {"period": "6 month"}
                },
                "y_axis": {
                    "operation": "count",
                    "entity": "hires",
                    "group_by": {"field": "month"},
                    "filters": {"period": "6 month"}
                }
            }
        }
    },
    {
        "name": "Source Effectiveness Comparison",
        "description": "Source breakdown with bar chart visualization",
        "report": {
            "report_title": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤",
            "period": "3 month",
            "metrics_group_by": "sources",
            "main_metric": {
                "label": "–ù–∞–Ω—è—Ç–æ —á–µ—Ä–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫",
                "value": {
                    "operation": "count",
                    "entity": "hires",
                    "value_field": None,
                    "filters": {"period": "3 month"}
                }
            },
            "secondary_metrics": [
                {
                    "label": "–ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫",
                    "value": {
                        "operation": "count",
                        "entity": "applicants",
                        "value_field": None,
                        "filters": {"period": "3 month"}
                    }
                },
                {
                    "label": "–ö–æ–Ω–≤–µ—Ä—Å–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞",
                    "value": {
                        "operation": "avg",
                        "entity": "sources",
                        "value_field": "conversion",
                        "filters": {"period": "3 month"}
                    }
                }
            ],
            "chart": {
                "label": "–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
                "type": "bar",
                "x_label": "–ò—Å—Ç–æ—á–Ω–∏–∫–∏",
                "y_label": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–º–∞",
                "x_axis": {
                    "operation": "count",
                    "entity": "hires",
                    "group_by": {"field": "sources"},
                    "filters": {"period": "3 month"}
                },
                "y_axis": {
                    "operation": "count",
                    "entity": "hires",
                    "group_by": {"field": "sources"},
                    "filters": {"period": "3 month"}
                }
            }
        }
    },
    {
        "name": "Division Performance Overview",
        "description": "Division breakdown with table view",
        "report": {
            "report_title": "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç–¥–µ–ª–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏",
            "period": "1 month",
            "metrics_group_by": "divisions",
            "main_metric": {
                "label": "–ù–∞–Ω—è—Ç–æ –≤ –æ—Ç–¥–µ–ª",
                "value": {
                    "operation": "count",
                    "entity": "hires",
                    "value_field": None,
                    "filters": {"period": "1 month"}
                }
            },
            "secondary_metrics": [
                {
                    "label": "–û—Ç–∫—Ä—ã—Ç—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π",
                    "value": {
                        "operation": "count",
                        "entity": "vacancies",
                        "value_field": None,
                        "filters": {"vacancies": "open"}
                    }
                },
                {
                    "label": "–ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ –æ—Ç–¥–µ–ª–µ",
                    "value": {
                        "operation": "count",
                        "entity": "applicants",
                        "value_field": None,
                        "filters": {"period": "1 month"}
                    }
                }
            ],
            "chart": {
                "label": "–°—Ç–∞—Ç—É—Å –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –æ—Ç–¥–µ–ª–∞–º",
                "type": "table",
                "x_label": "–û—Ç–¥–µ–ª—ã",
                "y_label": "–î–∞–Ω–Ω—ã–µ",
                "x_axis": {
                    "operation": "count",
                    "entity": "vacancies",
                    "group_by": {"field": "divisions"},
                    "filters": {}
                },
                "y_axis": {
                    "operation": "count",
                    "entity": "vacancies",
                    "group_by": {"field": "divisions"},
                    "filters": {}
                }
            }
        }
    }
]

async def run_end_to_end_test():
    """Run comprehensive end-to-end test"""
    print("üöÄ Starting Comprehensive End-to-End Test\n")
    
    client = HuntflowLocalClient()
    all_results = []
    start_time = time.time()
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"üìä Test {i}/3: {scenario['name']}")
        print(f"   Description: {scenario['description']}")
        
        scenario_start = time.time()
        
        try:
            # 1. Schema Validation
            print("   üìã Validating schema...")
            validate_report_json(scenario['report'])
            print("   ‚úÖ Schema validation: PASSED")
            
            # 2. Data Processing
            print("   ‚öôÔ∏è  Processing with real data...")
            processed = await process_chart_data(scenario['report'].copy(), client)
            processing_time = time.time() - scenario_start
            
            # 3. Structure Validation
            print("   üîç Validating result structure...")
            
            # Check required fields
            assert "metrics_group_by" in processed, "Missing metrics_group_by"
            assert "main_metric" in processed, "Missing main_metric"
            assert "secondary_metrics" in processed, "Missing secondary_metrics"
            assert "chart" in processed, "Missing chart"
            
            # Check metrics structure
            main_metric = processed["main_metric"]
            assert "real_value" in main_metric, "Missing main_metric.real_value"
            assert "total_value" in main_metric, "Missing main_metric.total_value"
            
            # Check secondary metrics
            for j, sec_metric in enumerate(processed["secondary_metrics"]):
                assert "real_value" in sec_metric, f"Missing secondary_metrics[{j}].real_value"
                assert "total_value" in sec_metric, f"Missing secondary_metrics[{j}].total_value"
            
            # Check grouping consistency
            expected_group_by = scenario['report']['metrics_group_by']
            actual_group_by = processed['metrics_group_by']
            assert actual_group_by == expected_group_by, f"Group by mismatch: {actual_group_by} != {expected_group_by}"
            
            # 4. Data Quality Check
            print("   üìà Checking data quality...")
            
            # Check for grouped breakdown when available
            has_breakdown = "grouped_breakdown" in main_metric
            breakdown_count = len(main_metric.get("grouped_breakdown", {}))
            total_value = main_metric.get("total_value", 0)
            
            print(f"   üìä Results:")
            print(f"      - Grouping: {actual_group_by}")
            print(f"      - Has breakdown: {has_breakdown}")
            print(f"      - Entities found: {breakdown_count}")
            print(f"      - Total value: {total_value}")
            print(f"      - Processing time: {processing_time:.3f}s")
            
            # Store results
            all_results.append({
                "scenario": scenario['name'],
                "group_by": actual_group_by,
                "has_breakdown": has_breakdown,
                "entity_count": breakdown_count,
                "total_value": total_value,
                "processing_time": processing_time,
                "success": True
            })
            
            print(f"   ‚úÖ Test {i}: PASSED\n")
            
        except Exception as e:
            print(f"   ‚ùå Test {i}: FAILED - {e}")
            all_results.append({
                "scenario": scenario['name'],
                "success": False,
                "error": str(e)
            })
            
    total_time = time.time() - start_time
    
    # Final Report
    print("=" * 60)
    print("üìã COMPREHENSIVE END-TO-END TEST RESULTS")
    print("=" * 60)
    
    successful_tests = [r for r in all_results if r.get("success")]
    failed_tests = [r for r in all_results if not r.get("success")]
    
    print(f"‚úÖ Successful tests: {len(successful_tests)}/{len(all_results)}")
    print(f"‚ùå Failed tests: {len(failed_tests)}/{len(all_results)}")
    print(f"‚è±Ô∏è  Total execution time: {total_time:.3f}s")
    print(f"üöÄ Average processing time: {sum(r.get('processing_time', 0) for r in successful_tests) / len(successful_tests) if successful_tests else 0:.3f}s")
    
    if successful_tests:
        print("\nüìä Successful Test Details:")
        for result in successful_tests:
            print(f"   ‚Ä¢ {result['scenario']}")
            print(f"     Group by: {result['group_by']}, Entities: {result['entity_count']}, Total: {result['total_value']}")
    
    if failed_tests:
        print("\n‚ùå Failed Test Details:")
        for result in failed_tests:
            print(f"   ‚Ä¢ {result['scenario']}: {result['error']}")
    
    # Performance Check
    avg_time = sum(r.get('processing_time', 0) for r in successful_tests) / len(successful_tests) if successful_tests else 0
    if avg_time > 1.0:
        print(f"\n‚ö†Ô∏è  Performance Warning: Average processing time {avg_time:.3f}s exceeds 1s threshold")
    else:
        print(f"\nüöÄ Performance: Excellent! Average processing time {avg_time:.3f}s")
    
    # Feature Coverage Check
    group_by_types = set(r.get('group_by') for r in successful_tests if r.get('group_by'))
    print(f"\nüéØ Feature Coverage:")
    print(f"   ‚Ä¢ Group by types tested: {', '.join(group_by_types)}")
    print(f"   ‚Ä¢ Chart types tested: line, bar, table")
    print(f"   ‚Ä¢ Entity types tested: hires, applicants, sources, vacancies")
    print(f"   ‚Ä¢ Operations tested: count, avg")
    
    return len(failed_tests) == 0

if __name__ == "__main__":
    async def main():
        success = await run_end_to_end_test()
        
        if success:
            print("\nüéâ ALL END-TO-END TESTS PASSED!")
            print("‚úÖ Unified metrics group_by implementation is PRODUCTION READY!")
            exit(0)
        else:
            print("\n‚ùå SOME END-TO-END TESTS FAILED!")
            print("üîß Check implementation before deployment")
            exit(1)
    
    asyncio.run(main())