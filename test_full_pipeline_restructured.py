#!/usr/bin/env python3
"""
Comprehensive Full Pipeline Test for Restructured Metrics Filter System
Tests the complete flow: Schema â†’ Processing â†’ Frontend â†’ Results
"""

import asyncio
import json
import time
from chart_data_processor import process_chart_data, validate_report_json
from huntflow_local_client import HuntflowLocalClient

# Test scenarios covering all major use cases with new structure
pipeline_test_scenarios = [
    {
        "name": "General Overview (Auto Breakdown)",
        "description": "Period-only filter should trigger automatic recruiter breakdown",
        "report": {
            "report_title": "ĞĞ±Ñ‰Ğ°Ñ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ Ñ Ğ½Ğ°Ğ¹Ğ¼Ğ¾Ğ¼",
            "metrics_filter": {
                "period": "6 month"
            },
            "main_metric": {
                "label": "ĞĞ°Ğ½ÑÑ‚Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸ĞºĞ¾Ğ²",
                "value": {
                    "operation": "count",
                    "entity": "hires"
                }
            },
            "secondary_metrics": [
                {
                    "label": "ĞšĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾",
                    "value": {
                        "operation": "count",
                        "entity": "applicants"
                    }
                },
                {
                    "label": "Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ°Ğ¹Ğ¼Ğ°",
                    "value": {
                        "operation": "avg",
                        "entity": "hires",
                        "value_field": "time_to_hire"
                    }
                }
            ],
            "chart": {
                "label": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ½Ğ°Ğ¹Ğ¼Ğ° Ğ¿Ğ¾ Ğ¼ĞµÑÑÑ†Ğ°Ğ¼",
                "type": "line",
                "x_label": "ĞœĞµÑÑÑ†",
                "y_label": "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ¹Ğ¼Ğ°",
                "x_axis": {
                    "operation": "count",
                    "entity": "hires",
                    "group_by": {"field": "month"}
                },
                "y_axis": {
                    "operation": "count",
                    "entity": "hires",
                    "group_by": {"field": "month"}
                }
            }
        },
        "expected_behavior": "auto_breakdown"
    },
    {
        "name": "Specific Recruiter Analysis",
        "description": "Specific recruiter filter should give aggregated results",
        "report": {
            "report_title": "Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¡Ğ°Ñ„Ğ¾Ğ½Ğ¾Ğ²Ğ°",
            "metrics_filter": {
                "period": "3 month",
                "recruiters": "55498"
            },
            "main_metric": {
                "label": "ĞĞ°Ğ½ÑÑ‚Ğ¾ Ğ¡Ğ°Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ğ¼",
                "value": {
                    "operation": "count",
                    "entity": "hires"
                }
            },
            "secondary_metrics": [
                {
                    "label": "ĞšĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ»",
                    "value": {
                        "operation": "count",
                        "entity": "applicants"
                    }
                },
                {
                    "label": "Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ°Ğ¹Ğ¼Ğ°",
                    "value": {
                        "operation": "avg",
                        "entity": "hires",
                        "value_field": "time_to_hire"
                    }
                }
            ],
            "chart": {
                "label": "Ğ¢Ñ€ĞµĞ½Ğ´ Ğ½Ğ°Ğ¹Ğ¼Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸",
                "type": "bar",
                "x_label": "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸",
                "y_label": "ĞšĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ñ‹",
                "x_axis": {
                    "operation": "count",
                    "entity": "applicants",
                    "group_by": {"field": "sources"}
                },
                "y_axis": {
                    "operation": "count",
                    "entity": "applicants",
                    "group_by": {"field": "sources"}
                }
            }
        },
        "expected_behavior": "aggregated"
    },
    {
        "name": "Division Performance Analysis",
        "description": "Specific division filter with table chart",
        "report": {
            "report_title": "Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ IT Ğ¾Ñ‚Ğ´ĞµĞ»Ğ°",
            "metrics_filter": {
                "period": "1 month",
                "divisions": "101"
            },
            "main_metric": {
                "label": "ĞĞ°Ğ½ÑÑ‚Ğ¾ Ğ² IT",
                "value": {
                    "operation": "count",
                    "entity": "hires"
                }
            },
            "secondary_metrics": [
                {
                    "label": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ Ğ² IT",
                    "value": {
                        "operation": "count",
                        "entity": "vacancies"
                    }
                },
                {
                    "label": "ĞšĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ² IT",
                    "value": {
                        "operation": "count",
                        "entity": "applicants"
                    }
                }
            ],
            "chart": {
                "label": "Ğ”ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼",
                "type": "table",
                "x_label": "Ğ­Ñ‚Ğ°Ğ¿Ñ‹",
                "y_label": "Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ",
                "x_axis": {
                    "operation": "count",
                    "entity": "applicants",
                    "group_by": {"field": "stages"}
                },
                "y_axis": {
                    "operation": "count",
                    "entity": "applicants",
                    "group_by": {"field": "stages"}
                }
            }
        },
        "expected_behavior": "aggregated"
    }
]

async def run_full_pipeline_test():
    """Run comprehensive full pipeline test with new metrics_filter structure"""
    print("ğŸš€ FULL PIPELINE TEST - Restructured Metrics Filter System\n")
    print("=" * 70)
    
    client = HuntflowLocalClient()
    all_results = []
    start_time = time.time()
    
    for i, scenario in enumerate(pipeline_test_scenarios, 1):
        print(f"\nğŸ“Š Pipeline Test {i}/3: {scenario['name']}")
        print(f"   ğŸ“ Description: {scenario['description']}")
        print(f"   ğŸ¯ Expected: {scenario['expected_behavior']}")
        
        scenario_start = time.time()
        
        try:
            # 1. Schema Validation
            print("   ğŸ“‹ Step 1: Schema validation...")
            validate_report_json(scenario['report'])
            print("   âœ… Schema validation: PASSED")
            
            # 2. Backend Processing
            print("   âš™ï¸  Step 2: Backend processing...")
            processed = await process_chart_data(scenario['report'].copy(), client)
            processing_time = time.time() - scenario_start
            
            # 3. Data Structure Analysis
            print("   ğŸ” Step 3: Analyzing results...")
            
            # Check metrics_filter
            metrics_filter = processed.get("metrics_filter", {})
            period = metrics_filter.get("period", "unknown")
            entity_filters = {k: v for k, v in metrics_filter.items() if k != "period" and v is not None}
            
            # Check main metric structure
            main_metric = processed.get("main_metric", {})
            has_breakdown = main_metric.get("is_grouped", False)
            total_value = main_metric.get("total_value", main_metric.get("real_value", 0))
            breakdown_count = 0  # Not stored anymore, but grouping tracked with is_grouped flag
            
            # Check chart data
            chart_data = processed.get("chart", {}).get("real_data", {})
            chart_labels = len(chart_data.get("labels", []))
            
            # 4. Behavior Validation
            print("   ğŸ¯ Step 4: Validating expected behavior...")
            
            behavior_correct = False
            if scenario["expected_behavior"] == "auto_breakdown":
                # Should have breakdown when no entity filters
                behavior_correct = (len(entity_filters) == 0 and has_breakdown)
                behavior_msg = f"Auto breakdown: {has_breakdown} (expected: True)"
            elif scenario["expected_behavior"] == "aggregated":
                # Should NOT have breakdown when specific entity filter
                behavior_correct = (len(entity_filters) > 0 and not has_breakdown)
                behavior_msg = f"Aggregated result: {not has_breakdown} (expected: True)"
            
            print(f"   ğŸ“Š Results Summary:")
            print(f"      - Period: {period}")
            print(f"      - Entity filters: {entity_filters}")
            print(f"      - Has breakdown: {has_breakdown}")
            print(f"      - Is grouped: {has_breakdown}")
            print(f"      - Total value: {total_value}")
            print(f"      - Chart data points: {chart_labels}")
            print(f"      - Processing time: {processing_time:.3f}s")
            print(f"      - Behavior check: {behavior_msg}")
            
            # Store results
            all_results.append({
                "scenario": scenario['name'],
                "period": period,
                "entity_filters": len(entity_filters),
                "has_breakdown": has_breakdown,
                "is_grouped": has_breakdown,
                "total_value": total_value,
                "chart_points": chart_labels,
                "processing_time": processing_time,
                "behavior_correct": behavior_correct,
                "success": True
            })
            
            status = "âœ… PASSED" if behavior_correct else "âš ï¸  BEHAVIOR MISMATCH"
            print(f"   {status}")
            
        except Exception as e:
            print(f"   âŒ FAILED: {e}")
            all_results.append({
                "scenario": scenario['name'],
                "success": False,
                "error": str(e),
                "processing_time": time.time() - scenario_start
            })
            
    total_time = time.time() - start_time
    
    # Final Analysis
    print("\n" + "=" * 70)
    print("ğŸ“‹ FULL PIPELINE TEST RESULTS")
    print("=" * 70)
    
    successful_tests = [r for r in all_results if r.get("success")]
    failed_tests = [r for r in all_results if not r.get("success")]
    behavior_correct_tests = [r for r in successful_tests if r.get("behavior_correct")]
    
    print(f"âœ… Successful tests: {len(successful_tests)}/{len(all_results)}")
    print(f"ğŸ¯ Correct behavior: {len(behavior_correct_tests)}/{len(successful_tests)}")
    print(f"âŒ Failed tests: {len(failed_tests)}/{len(all_results)}")
    print(f"â±ï¸  Total execution time: {total_time:.3f}s")
    
    if successful_tests:
        avg_time = sum(r.get('processing_time', 0) for r in successful_tests) / len(successful_tests)
        print(f"ğŸš€ Average processing time: {avg_time:.3f}s")
        
        print("\nğŸ“Š Detailed Results:")
        for result in successful_tests:
            status = "ğŸ¯" if result.get('behavior_correct') else "âš ï¸"
            print(f"   {status} {result['scenario']}")
            print(f"      Filters: {result['entity_filters']}, Grouped: {result['is_grouped']}, Total: {result['total_value']}")
    
    if failed_tests:
        print("\nâŒ Failed Tests:")
        for result in failed_tests:
            print(f"   â€¢ {result['scenario']}: {result['error']}")
    
    # Performance Assessment
    if successful_tests:
        avg_time = sum(r.get('processing_time', 0) for r in successful_tests) / len(successful_tests)
        if avg_time > 1.0:
            print(f"\nâš ï¸  Performance Warning: Average time {avg_time:.3f}s exceeds 1s")
        else:
            print(f"\nğŸš€ Performance: Excellent! Average time {avg_time:.3f}s")
    
    # Feature Coverage Assessment
    print(f"\nğŸ¯ Feature Coverage Assessment:")
    print(f"   â€¢ Automatic breakdown logic: Tested")
    print(f"   â€¢ Specific entity filtering: Tested") 
    print(f"   â€¢ Chart independence: Tested")
    print(f"   â€¢ All chart types: line, bar, table")
    print(f"   â€¢ All operations: count, avg")
    print(f"   â€¢ Multiple entities: hires, applicants, vacancies")
    
    return len(failed_tests) == 0 and len(behavior_correct_tests) == len(successful_tests)

if __name__ == "__main__":
    async def main():
        success = await run_full_pipeline_test()
        
        if success:
            print("\nğŸ‰ FULL PIPELINE TEST PASSED!")
            print("âœ… Restructured metrics_filter system is PRODUCTION READY!")
            exit(0)
        else:
            print("\nâŒ FULL PIPELINE TEST FAILED!")
            print("ğŸ”§ Check implementation before deployment")
            exit(1)
    
    asyncio.run(main())