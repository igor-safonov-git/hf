#!/usr/bin/env python3
"""
Comprehensive Full Pipeline Test for Restructured Metrics Filter System
Tests the complete flow: Schema ‚Üí Processing ‚Üí Frontend ‚Üí Results
"""

import asyncio
import json
import time
from chart_data_processor import process_chart_data, validate_report_json
from huntflow_local_client import HuntflowLocalClient

# Test scenarios covering all major use cases with new structure
pipeline_test_scenarios = [
    {
        "name": "General Overview (Auto Breakdown)",
        "description": "Period-only filter should trigger automatic recruiter breakdown",
        "report": {
            "report_title": "–û–±—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è —Å –Ω–∞–π–º–æ–º",
            "metrics_filter": {
                "period": "6 month"
            },
            "main_metric": {
                "label": "–ù–∞–Ω—è—Ç–æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤",
                "value": {
                    "operation": "count",
                    "entity": "hires"
                }
            },
            "secondary_metrics": [
                {
                    "label": "–ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ",
                    "value": {
                        "operation": "count",
                        "entity": "applicants"
                    }
                },
                {
                    "label": "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞–π–º–∞",
                    "value": {
                        "operation": "avg",
                        "entity": "hires",
                        "value_field": "time_to_hire"
                    }
                }
            ],
            "chart": {
                "label": "–î–∏–Ω–∞–º–∏–∫–∞ –Ω–∞–π–º–∞ –ø–æ –º–µ—Å—è—Ü–∞–º",
                "type": "line",
                "x_label": "–ú–µ—Å—è—Ü",
                "y_label": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–º–∞",
                "x_axis": {
                    "operation": "count",
                    "entity": "hires",
                    "group_by": {"field": "month"}
                },
                "y_axis": {
                    "operation": "count",
                    "entity": "hires",
                    "group_by": {"field": "month"}
                }
            }
        },
        "expected_behavior": "auto_breakdown"
    },
    {
        "name": "Specific Recruiter Analysis",
        "description": "Specific recruiter filter should give aggregated results",
        "report": {
            "report_title": "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –°–∞—Ñ–æ–Ω–æ–≤–∞",
            "metrics_filter": {
                "period": "3 month",
                "recruiters": "55498"
            },
            "main_metric": {
                "label": "–ù–∞–Ω—è—Ç–æ –°–∞—Ñ–æ–Ω–æ–≤—ã–º",
                "value": {
                    "operation": "count",
                    "entity": "hires"
                }
            },
            "secondary_metrics": [
                {
                    "label": "–ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–æ–±–∞–≤–∏–ª",
                    "value": {
                        "operation": "count",
                        "entity": "applicants"
                    }
                },
                {
                    "label": "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞–π–º–∞",
                    "value": {
                        "operation": "avg",
                        "entity": "hires",
                        "value_field": "time_to_hire"
                    }
                }
            ],
            "chart": {
                "label": "–¢—Ä–µ–Ω–¥ –Ω–∞–π–º–∞ –∫–æ–º–ø–∞–Ω–∏–∏",
                "type": "bar",
                "x_label": "–ò—Å—Ç–æ—á–Ω–∏–∫–∏",
                "y_label": "–ö–∞–Ω–¥–∏–¥–∞—Ç—ã",
                "x_axis": {
                    "operation": "count",
                    "entity": "applicants",
                    "group_by": {"field": "sources"}
                },
                "y_axis": {
                    "operation": "count",
                    "entity": "applicants",
                    "group_by": {"field": "sources"}
                }
            }
        },
        "expected_behavior": "aggregated"
    },
    {
        "name": "Division Performance Analysis",
        "description": "Specific division filter with table chart",
        "report": {
            "report_title": "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã IT –æ—Ç–¥–µ–ª–∞",
            "metrics_filter": {
                "period": "1 month",
                "divisions": "101"
            },
            "main_metric": {
                "label": "–ù–∞–Ω—è—Ç–æ –≤ IT",
                "value": {
                    "operation": "count",
                    "entity": "hires"
                }
            },
            "secondary_metrics": [
                {
                    "label": "–û—Ç–∫—Ä—ã—Ç—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –≤ IT",
                    "value": {
                        "operation": "count",
                        "entity": "vacancies"
                    }
                },
                {
                    "label": "–ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ IT",
                    "value": {
                        "operation": "count",
                        "entity": "applicants"
                    }
                }
            ],
            "chart": {
                "label": "–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —ç—Ç–∞–ø–∞–º",
                "type": "table",
                "x_label": "–≠—Ç–∞–ø—ã",
                "y_label": "–î–∞–Ω–Ω—ã–µ",
                "x_axis": {
                    "operation": "count",
                    "entity": "applicants",
                    "group_by": {"field": "stages"}
                },
                "y_axis": {
                    "operation": "count",
                    "entity": "applicants",
                    "group_by": {"field": "stages"}
                }
            }
        },
        "expected_behavior": "aggregated"
    }
]

async def run_full_pipeline_test():
    """Run comprehensive full pipeline test with new metrics_filter structure"""
    print("üöÄ FULL PIPELINE TEST - Restructured Metrics Filter System\n")
    print("=" * 70)
    
    client = HuntflowLocalClient()
    all_results = []
    start_time = time.time()
    
    for i, scenario in enumerate(pipeline_test_scenarios, 1):
        print(f"\nüìä Pipeline Test {i}/3: {scenario['name']}")
        print(f"   üìù Description: {scenario['description']}")
        print(f"   üéØ Expected: {scenario['expected_behavior']}")
        
        scenario_start = time.time()
        
        try:
            # 1. Schema Validation
            print("   üìã Step 1: Schema validation...")
            validate_report_json(scenario['report'])
            print("   ‚úÖ Schema validation: PASSED")
            
            # 2. Backend Processing
            print("   ‚öôÔ∏è  Step 2: Backend processing...")
            processed = await process_chart_data(scenario['report'].copy(), client)
            processing_time = time.time() - scenario_start
            
            # 3. Data Structure Analysis
            print("   üîç Step 3: Analyzing results...")
            
            # Check metrics_filter
            metrics_filter = processed.get("metrics_filter", {})
            period = metrics_filter.get("period", "unknown")
            entity_filters = {k: v for k, v in metrics_filter.items() if k != "period" and v is not None}
            
            # Check main metric structure
            main_metric = processed.get("main_metric", {})
            has_breakdown = main_metric.get("is_grouped", False)
            total_value = main_metric.get("total_value", main_metric.get("real_value", 0))
            breakdown_count = 0  # Not stored anymore, but grouping tracked with is_grouped flag
            
            # Check chart data
            chart_data = processed.get("chart", {}).get("real_data", {})
            chart_labels = len(chart_data.get("labels", []))
            
            # 4. Behavior Validation
            print("   üéØ Step 4: Validating expected behavior...")
            
            behavior_correct = False
            if scenario["expected_behavior"] == "auto_breakdown":
                # Should have breakdown when no entity filters
                behavior_correct = (len(entity_filters) == 0 and has_breakdown)
                behavior_msg = f"Auto breakdown: {has_breakdown} (expected: True)"
            elif scenario["expected_behavior"] == "aggregated":
                # Should NOT have breakdown when specific entity filter
                behavior_correct = (len(entity_filters) > 0 and not has_breakdown)
                behavior_msg = f"Aggregated result: {not has_breakdown} (expected: True)"
            
            print(f"   üìä Results Summary:")
            print(f"      - Period: {period}")
            print(f"      - Entity filters: {entity_filters}")
            print(f"      - Has breakdown: {has_breakdown}")
            print(f"      - Is grouped: {has_breakdown}")
            print(f"      - Total value: {total_value}")
            print(f"      - Chart data points: {chart_labels}")
            print(f"      - Processing time: {processing_time:.3f}s")
            print(f"      - Behavior check: {behavior_msg}")
            
            # Store results
            all_results.append({
                "scenario": scenario['name'],
                "period": period,
                "entity_filters": len(entity_filters),
                "has_breakdown": has_breakdown,
                "is_grouped": has_breakdown,
                "total_value": total_value,
                "chart_points": chart_labels,
                "processing_time": processing_time,
                "behavior_correct": behavior_correct,
                "success": True
            })
            
            status = "‚úÖ PASSED" if behavior_correct else "‚ö†Ô∏è  BEHAVIOR MISMATCH"
            print(f"   {status}")
            
        except Exception as e:
            print(f"   ‚ùå FAILED: {e}")
            all_results.append({
                "scenario": scenario['name'],
                "success": False,
                "error": str(e),
                "processing_time": time.time() - scenario_start
            })
            
    total_time = time.time() - start_time
    
    # Final Analysis
    print("\n" + "=" * 70)
    print("üìã FULL PIPELINE TEST RESULTS")
    print("=" * 70)
    
    successful_tests = [r for r in all_results if r.get("success")]
    failed_tests = [r for r in all_results if not r.get("success")]
    behavior_correct_tests = [r for r in successful_tests if r.get("behavior_correct")]
    
    print(f"‚úÖ Successful tests: {len(successful_tests)}/{len(all_results)}")
    print(f"üéØ Correct behavior: {len(behavior_correct_tests)}/{len(successful_tests)}")
    print(f"‚ùå Failed tests: {len(failed_tests)}/{len(all_results)}")
    print(f"‚è±Ô∏è  Total execution time: {total_time:.3f}s")
    
    if successful_tests:
        avg_time = sum(r.get('processing_time', 0) for r in successful_tests) / len(successful_tests)
        print(f"üöÄ Average processing time: {avg_time:.3f}s")
        
        print("\nüìä Detailed Results:")
        for result in successful_tests:
            status = "üéØ" if result.get('behavior_correct') else "‚ö†Ô∏è"
            print(f"   {status} {result['scenario']}")
            print(f"      Filters: {result['entity_filters']}, Grouped: {result['is_grouped']}, Total: {result['total_value']}")
    
    if failed_tests:
        print("\n‚ùå Failed Tests:")
        for result in failed_tests:
            print(f"   ‚Ä¢ {result['scenario']}: {result['error']}")
    
    # Performance Assessment
    if successful_tests:
        avg_time = sum(r.get('processing_time', 0) for r in successful_tests) / len(successful_tests)
        if avg_time > 1.0:
            print(f"\n‚ö†Ô∏è  Performance Warning: Average time {avg_time:.3f}s exceeds 1s")
        else:
            print(f"\nüöÄ Performance: Excellent! Average time {avg_time:.3f}s")
    
    # Feature Coverage Assessment
    print(f"\nüéØ Feature Coverage Assessment:")
    print(f"   ‚Ä¢ Automatic breakdown logic: Tested")
    print(f"   ‚Ä¢ Specific entity filtering: Tested") 
    print(f"   ‚Ä¢ Chart independence: Tested")
    print(f"   ‚Ä¢ All chart types: line, bar, table")
    print(f"   ‚Ä¢ All operations: count, avg")
    print(f"   ‚Ä¢ Multiple entities: hires, applicants, vacancies")
    
    return len(failed_tests) == 0 and len(behavior_correct_tests) == len(successful_tests)

if __name__ == "__main__":
    async def main():
        success = await run_full_pipeline_test()
        
        if success:
            print("\nüéâ FULL PIPELINE TEST PASSED!")
            print("‚úÖ Restructured metrics_filter system is PRODUCTION READY!")
            exit(0)
        else:
            print("\n‚ùå FULL PIPELINE TEST FAILED!")
            print("üîß Check implementation before deployment")
            exit(1)
    
    asyncio.run(main())